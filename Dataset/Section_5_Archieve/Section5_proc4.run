#!/bin/bash
#SBATCH --nodes=1                     # Number of nodes
#SBATCH --ntasks-per-node=4           # Number of MPI tasks per node
#SBATCH --cpus-per-task=1             # Number of CPUs per task
#SBATCH --time=1:20:00                # Time limit (hh:mm:ss)
#SBATCH --account=math-505            # Account name
#SBATCH --qos=serial                  # QoS
#SBATCH --output=./results/mpi_Section5_%j.txt   # Output file

# Activate Python virtual environment and load necessary modules
source ~/venvs/myenv/bin/activate
module load gcc openmpi python py-mpi4py

# Number of repetitions
NUM_RUNS=5

# Full path to Python script
SCRIPT_PATH=/home/rutan/Math505HPC/Project2/Section5.py

# Store the SLURM job ID
JOB_ID=$SLURM_JOB_ID

# Loop to run the MPI program repeatedly
for i in $(seq 1 $NUM_RUNS)
do
    echo "Starting run $i at $(date)" >> ./results/mpi_Section5_${JOB_ID}.txt
    # Corrected srun command: run the script with the virtual environment Python interpreter
    srun --mpi=pmix ~/venvs/myenv/bin/python $SCRIPT_PATH
    echo "Finished run $i at $(date)" >> ./results/mpi_Section5_${JOB_ID}.txt
done
