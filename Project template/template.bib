
@misc{chenakkod_optimal_2024,
	title = {Optimal {Embedding} {Dimension} for {Sparse} {Subspace} {Embeddings}},
	url = {http://arxiv.org/abs/2311.10680},
	abstract = {A random \$m{\textbackslash}times n\$ matrix \$S\$ is an oblivious subspace embedding (OSE) with parameters \${\textbackslash}epsilon{\textgreater}0\$, \${\textbackslash}delta{\textbackslash}in(0,1/3)\$ and \$d{\textbackslash}leq m{\textbackslash}leq n\$, if for any \$d\$-dimensional subspace \$W{\textbackslash}subseteq R{\textasciicircum}n\$, \$P{\textbackslash}big({\textbackslash},{\textbackslash}forall\_\{x{\textbackslash}in W\}{\textbackslash} (1+{\textbackslash}epsilon){\textasciicircum}\{-1\}{\textbackslash}{\textbar}x{\textbackslash}{\textbar}{\textbackslash}leq{\textbackslash}{\textbar}Sx{\textbackslash}{\textbar}{\textbackslash}leq (1+{\textbackslash}epsilon){\textbackslash}{\textbar}x{\textbackslash}{\textbar}{\textbackslash},{\textbackslash}big){\textbackslash}geq 1-{\textbackslash}delta.\$ It is known that the embedding dimension of an OSE must satisfy \$m{\textbackslash}geq d\$, and for any \${\textbackslash}theta {\textgreater} 0\$, a Gaussian embedding matrix with \$m{\textbackslash}geq (1+{\textbackslash}theta) d\$ is an OSE with \${\textbackslash}epsilon = O\_{\textbackslash}theta(1)\$. However, such optimal embedding dimension is not known for other embeddings. Of particular interest are sparse OSEs, having \$s{\textbackslash}ll m\$ non-zeros per column, with applications to problems such as least squares regression and low-rank approximation. We show that, given any \${\textbackslash}theta {\textgreater} 0\$, an \$m{\textbackslash}times n\$ random matrix \$S\$ with \$m{\textbackslash}geq (1+{\textbackslash}theta)d\$ consisting of randomly sparsified \${\textbackslash}pm1/{\textbackslash}sqrt s\$ entries and having \$s= O({\textbackslash}log{\textasciicircum}4(d))\$ non-zeros per column, is an oblivious subspace embedding with \${\textbackslash}epsilon = O\_\{{\textbackslash}theta\}(1)\$. Our result addresses the main open question posed by Nelson and Nguyen (FOCS 2013), who conjectured that sparse OSEs can achieve \$m=O(d)\$ embedding dimension, and it improves on \$m=O(d{\textbackslash}log(d))\$ shown by Cohen (SODA 2016). We use this to construct the first oblivious subspace embedding with \$O(d)\$ embedding dimension that can be applied faster than current matrix multiplication time, and to obtain an optimal single-pass algorithm for least squares regression. We further extend our results to Leverage Score Sparsification (LESS), which is a recently introduced non-oblivious embedding technique. We use LESS to construct the first subspace embedding with low distortion \${\textbackslash}epsilon=o(1)\$ and optimal embedding dimension \$m=O(d/{\textbackslash}epsilon{\textasciicircum}2)\$ that can be applied in current matrix multiplication time.},
	urldate = {2024-06-12},
	publisher = {arXiv},
	author = {Chenakkod, Shabarish and Dereziński, Michał and Dong, Xiaoyu and Rudelson, Mark},
	month = jun,
	year = {2024},
	note = {arXiv:2311.10680 [cs, math, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/NR6VKNG9/Chenakkod et al. - 2024 - Optimal Embedding Dimension for Sparse Subspace Em.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/4LT72I6I/2311.html:text/html},
}


@misc{brailovskaya_universality_2023,
	title = {Universality and sharp matrix concentration inequalities},
	url = {http://arxiv.org/abs/2201.05142},
	abstract = {We show that, under mild assumptions, the spectrum of a sum of independent random matrices is close to that of the Gaussian random matrix whose entries have the same mean and covariance. This nonasymptotic universality principle yields sharp matrix concentration inequalities for general sums of independent random matrices when combined with the Gaussian theory of Bandeira, Boedihardjo, and Van Handel. A key feature of the resulting theory is that it is applicable to a remarkably broad class of random matrix models that may have highly nonhomogeneous and dependent entries, which can be far outside the mean-field situation considered in classical random matrix theory. We illustrate the theory in applications to random graphs, matrix concentration inequalities for smallest singular values, sample covariance matrices, strong asymptotic freeness, and phase transitions in spiked models.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Brailovskaya, Tatiana and van Handel, Ramon},
	month = aug,
	year = {2023},
	note = {arXiv:2201.05142 [math]},
	keywords = {60B20, 60E15, 46L53, 46L54, 15B52, Mathematics - Functional Analysis, Mathematics - Operator Algebras, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/BCJS8V6N/Brailovskaya and van Handel - 2023 - Universality and sharp matrix concentration inequa.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/TI4SD6JV/2201.html:text/html},
}

@misc{nelson_lower_2013,
	title = {Lower bounds for oblivious subspace embeddings},
	url = {http://arxiv.org/abs/1308.3280},
	abstract = {An oblivious subspace embedding (OSE) for some eps, delta in (0,1/3) and d {\textless}= m {\textless}= n is a distribution D over R{\textasciicircum}\{m x n\} such that for any linear subspace W of R{\textasciicircum}n of dimension d, Pr\_\{Pi {\textasciitilde} D\}(for all x in W, (1-eps) {\textbar}x{\textbar}\_2 {\textless}= {\textbar}Pi x{\textbar}\_2 {\textless}= (1+eps){\textbar}x{\textbar}\_2) {\textgreater}= 1 - delta. We prove that any OSE with delta {\textless} 1/3 must have m = Omega((d + log(1/delta))/eps{\textasciicircum}2), which is optimal. Furthermore, if every Pi in the support of D is sparse, having at most s non-zero entries per column, then we show tradeoff lower bounds between m and s.},
	urldate = {2024-05-27},
	publisher = {arXiv},
	author = {Nelson, Jelani and Nguyen, Huy L.},
	month = aug,
	year = {2013},
	note = {arXiv:1308.3280 [cs, math]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Discrete Mathematics, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/N8T7XAKF/Nelson and Nguyen - 2013 - Lower bounds for oblivious subspace embeddings.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/5GK68ZFX/1308.html:text/html},
}

@misc{tropp_improved_2011,
	title = {Improved analysis of the subsampled randomized {Hadamard} transform},
	url = {http://arxiv.org/abs/1011.1595},
	abstract = {This paper presents an improved analysis of a structured dimension-reduction map called the subsampled randomized Hadamard transform. This argument demonstrates that the map preserves the Euclidean geometry of an entire subspace of vectors. The new proof is much simpler than previous approaches, and it offers---for the first time---optimal constants in the estimate on the number of dimensions required for the embedding.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Tropp, Joel A.},
	month = jul,
	year = {2011},
	note = {arXiv:1011.1595 [cs, math]},
	keywords = {15B52, Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/IU3LDXDE/Tropp - 2011 - Improved analysis of the subsampled randomized Had.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/RPGAKEI6/1011.html:text/html},
}

@misc{balabanov_block_2022,
	title = {Block subsampled randomized {Hadamard} transform for low-rank approximation on distributed architectures},
	url = {http://arxiv.org/abs/2210.11295},
	abstract = {This article introduces a novel structured random matrix composed blockwise from subsampled randomized Hadamard transforms (SRHTs). The block SRHT is expected to outperform well-known dimension reduction maps, including SRHT and Gaussian matrices, on distributed architectures with not too many cores compared to the dimension. We prove that a block SRHT with enough rows is an oblivious subspace embedding, i.e., an approximate isometry for an arbitrary low-dimensional subspace with high probability. Our estimate of the required number of rows is similar to that of the standard SRHT. This suggests that the two transforms should provide the same accuracy of approximation in the algorithms. The block SRHT can be readily incorporated into randomized methods, for instance to compute a low-rank approximation of a large-scale matrix. For completeness, we revisit some common randomized approaches for this problem such as Randomized Singular Value Decomposition and Nystr{\textbackslash}"\{o\}m approximation, with a discussion of their accuracy and implementation on distributed architectures.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Balabanov, Oleg and Beaupere, Matthias and Grigori, Laura and Lederer, Victor},
	month = oct,
	year = {2022},
	note = {arXiv:2210.11295 [cs, math]},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/GKXJGWQ7/Balabanov et al. - 2022 - Block subsampled randomized Hadamard transform for.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/C8NHX2WX/2210.html:text/html},
}

@misc{nelson_osnap_2012,
	title = {{OSNAP}: {Faster} numerical linear algebra algorithms via sparser subspace embeddings},
	shorttitle = {{OSNAP}},
	url = {http://arxiv.org/abs/1211.1002},
	abstract = {An "oblivious subspace embedding (OSE)" given some parameters eps,d is a distribution D over matrices B in R{\textasciicircum}\{m x n\} such that for any linear subspace W in R{\textasciicircum}n with dim(W) = d it holds that Pr\_\{B {\textasciitilde} D\}(forall x in W {\textbar}{\textbar}B x{\textbar}{\textbar}\_2 in (1 +/- eps){\textbar}{\textbar}x{\textbar}{\textbar}\_2) {\textgreater} 2/3 We show an OSE exists with m = O(d{\textasciicircum}2/eps{\textasciicircum}2) and where every B in the support of D has exactly s=1 non-zero entries per column. This improves previously best known bound in [Clarkson-Woodruff, arXiv:1207.6365]. Our quadratic dependence on d is optimal for any OSE with s=1 [Nelson-Nguyen, 2012]. We also give two OSE's, which we call Oblivious Sparse Norm-Approximating Projections (OSNAPs), that both allow the parameter settings m = {\textbackslash}{\textasciitilde}O(d/eps{\textasciicircum}2) and s = polylog(d)/eps, or m = O(d{\textasciicircum}\{1+gamma\}/eps{\textasciicircum}2) and s=O(1/eps) for any constant gamma{\textgreater}0. This m is nearly optimal since m {\textgreater}= d is required simply to no non-zero vector of W lands in the kernel of B. These are the first constructions with m=o(d{\textasciicircum}2) to have s=o(d). In fact, our OSNAPs are nothing more than the sparse Johnson-Lindenstrauss matrices of [Kane-Nelson, SODA 2012]. Our analyses all yield OSE's that are sampled using either O(1)-wise or O(log d)-wise independent hash functions, which provides some efficiency advantages over previous work for turnstile streaming applications. Our main result is essentially a Bai-Yin type theorem in random matrix theory and is likely to be of independent interest: i.e. we show that for any U in R{\textasciicircum}\{n x d\} with orthonormal columns and random sparse B, all singular values of BU lie in [1-eps, 1+eps] with good probability. Plugging OSNAPs into known algorithms for numerical linear algebra problems such as approximate least squares regression, low rank approximation, and approximating leverage scores implies faster algorithms for all these problems.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Nelson, Jelani and Nguyen, Huy L.},
	month = nov,
	year = {2012},
	note = {arXiv:1211.1002 [cs, math]},
	keywords = {Computer Science - Data Structures and Algorithms, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/QX3CLJM9/Nelson and Nguyen - 2012 - OSNAP Faster numerical linear algebra algorithms .pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/ZKBWJYVA/1211.html:text/html},
}

@article{tropp_user-friendly_2012,
	title = {User-friendly tail bounds for sums of random matrices},
	volume = {12},
	issn = {1615-3375, 1615-3383},
	url = {http://arxiv.org/abs/1004.4389},
	doi = {10.1007/s10208-011-9099-z},
	abstract = {This paper presents new probability inequalities for sums of independent, random, self-adjoint matrices. These results place simple and easily verifiable hypotheses on the summands, and they deliver strong conclusions about the large-deviation behavior of the maximum eigenvalue of the sum. Tail bounds for the norm of a sum of random rectangular matrices follow as an immediate corollary. The proof techniques also yield some information about matrix-valued martingales. In other words, this paper provides noncommutative generalizations of the classical bounds associated with the names Azuma, Bennett, Bernstein, Chernoff, Hoeffding, and McDiarmid. The matrix inequalities promise the same diversity of application, ease of use, and strength of conclusion that have made the scalar inequalities so valuable.},
	number = {4},
	urldate = {2024-06-05},
	journal = {Foundations of Computational Mathematics},
	author = {Tropp, Joel A.},
	month = aug,
	year = {2012},
	note = {arXiv:1004.4389 [math]},
	keywords = {Mathematics - Probability, Primary: 60B20. Secondary: 60F10, 60G50, 60G42},
	pages = {389--434},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/LUN9WQDJ/Tropp - 2012 - User-friendly tail bounds for sums of random matri.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/ZDCZ3PIQ/1004.html:text/html},
}

@article{tropp_practical_2017,
	title = {Practical {Sketching} {Algorithms} for {Low}-{Rank} {Matrix} {Approximation}},
	volume = {38},
	issn = {0895-4798, 1095-7162},
	url = {https://epubs.siam.org/doi/10.1137/17M1111590},
	doi = {10.1137/17M1111590},
	language = {en},
	number = {4},
	urldate = {2024-06-05},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Tropp, Joel A. and Yurtsever, Alp and Udell, Madeleine and Cevher, Volkan},
	month = jan,
	year = {2017},
	pages = {1454--1485},
	file = {Accepted Version:/Users/mmartine/Zotero/storage/B3IN9RAV/Tropp et al. - 2017 - Practical Sketching Algorithms for Low-Rank Matrix.pdf:application/pdf},
}

@misc{halko_finding_2010,
	title = {Finding structure with randomness: {Probabilistic} algorithms for constructing approximate matrix decompositions},
	shorttitle = {Finding structure with randomness},
	url = {http://arxiv.org/abs/0909.4061},
	abstract = {Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, speed, and robustness. These claims are supported by extensive numerical experiments and a detailed error analysis.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A.},
	month = dec,
	year = {2010},
	note = {arXiv:0909.4061 [math]},
	keywords = {Mathematics - Numerical Analysis, Mathematics - Probability},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/T5KVWNQP/Halko et al. - 2010 - Finding structure with randomness Probabilistic a.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/KD8CK6NC/0909.html:text/html},
}

@misc{cartis_hashing_2021,
	title = {Hashing embeddings of optimal dimension, with applications to linear least squares},
	url = {http://arxiv.org/abs/2105.11815},
	abstract = {The aim of this paper is two-fold: firstly, to present subspace embedding properties for \$s\$-hashing sketching matrices, with \$s{\textbackslash}geq 1\$, that are optimal in the projection dimension \$m\$ of the sketch, namely, \$m={\textbackslash}mathcal\{O\}(d)\$, where \$d\$ is the dimension of the subspace. A diverse set of results are presented that address the case when the input matrix has sufficiently low coherence (thus removing the \${\textbackslash}log{\textasciicircum}2 d\$ factor dependence in \$m\$, in the low-coherence result of Bourgain et al (2015) at the expense of a smaller coherence requirement); how this coherence changes with the number \$s\$ of column nonzeros (allowing a scaling of \${\textbackslash}sqrt\{s\}\$ of the coherence bound), or is reduced through suitable transformations (when considering hashed -- instead of subsampled -- coherence reducing transformations such as randomised Hadamard). Secondly, we apply these general hashing sketching results to the special case of Linear Least Squares (LLS), and develop Ski-LLS, a generic software package for these problems, that builds upon and improves the Blendenpik solver on dense input and the (sequential) LSRN performance on sparse problems. In addition to the hashing sketching improvements, we add suitable linear algebra tools for rank-deficient and for sparse problems that lead Ski-LLS to outperform not only sketching-based routines on randomly generated input, but also state of the art direct solver SPQR and iterative code HSL on certain subsets of the sparse Florida matrix collection; namely, on least squares problems that are significantly overdetermined, or moderately sparse, or difficult.},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Cartis, Coralia and Fiala, Jan and Shao, Zhen},
	month = may,
	year = {2021},
	note = {arXiv:2105.11815 [cs, math, stat]},
	keywords = {65K05, 93E24, 65F08, 65F10, 65F20, 65F50, 62J05, Mathematics - Numerical Analysis, Mathematics - Optimization and Control, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/U6ADKIY6/Cartis et al. - 2021 - Hashing embeddings of optimal dimension, with appl.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/MHGS3T22/2105.html:text/html},
}



@article{rokhlin_fast_2008,
	title = {A fast randomized algorithm for overdetermined linear least-squares regression},
	volume = {105},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.0804869105},
	doi = {10.1073/pnas.0804869105},
	language = {en},
	number = {36},
	urldate = {2024-07-04},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Rokhlin, Vladimir and Tygert, Mark},
	month = sep,
	year = {2008},
	pages = {13212--13217},
	file = {Full Text:/Users/mmartine/Zotero/storage/7RP9TE9R/Rokhlin and Tygert - 2008 - A fast randomized algorithm for overdetermined lin.pdf:application/pdf},
}


@article{ailon_fast_2009,
	title = {The {Fast} {Johnson}–{Lindenstrauss} {Transform} and {Approximate} {Nearest} {Neighbors}},
	volume = {39},
	issn = {0097-5397, 1095-7111},
	url = {http://epubs.siam.org/doi/10.1137/060673096},
	doi = {10.1137/060673096},
	language = {en},
	number = {1},
	urldate = {2024-07-04},
	journal = {SIAM Journal on Computing},
	author = {Ailon, Nir and Chazelle, Bernard},
	month = jan,
	year = {2009},
	pages = {302--322},
	file = {Submitted Version:/Users/mmartine/Zotero/storage/5INFJPPZ/Ailon and Chazelle - 2009 - The Fast Johnson–Lindenstrauss Transform and Appro.pdf:application/pdf},
}


@inproceedings{nguyen_fast_2009,
	address = {Bethesda MD USA},
	title = {A fast and efficient algorithm for low-rank approximation of a matrix},
	isbn = {978-1-60558-506-2},
	url = {https://dl.acm.org/doi/10.1145/1536414.1536446},
	doi = {10.1145/1536414.1536446},
	language = {en},
	urldate = {2024-07-04},
	booktitle = {Proceedings of the forty-first annual {ACM} symposium on {Theory} of computing},
	publisher = {ACM},
	author = {Nguyen, Nam H. and Do, Thong T. and Tran, Trac D.},
	month = may,
	year = {2009},
	pages = {215--224},
}



@misc{ailon_almost_2010,
	title = {Almost {Optimal} {Unrestricted} {Fast} {Johnson}-{Lindenstrauss} {Transform}},
	url = {http://arxiv.org/abs/1005.5513},
	abstract = {The problems of random projections and sparse reconstruction have much in common and individually received much attention. Surprisingly, until now they progressed in parallel and remained mostly separate. Here, we employ new tools from probability in Banach spaces that were successfully used in the context of sparse reconstruction to advance on an open problem in random pojection. In particular, we generalize and use an intricate result by Rudelson and Vershynin for sparse reconstruction which uses Dudley's theorem for bounding Gaussian processes. Our main result states that any set of \$N = {\textbackslash}exp({\textbackslash}tilde\{O\}(n))\$ real vectors in \$n\$ dimensional space can be linearly mapped to a space of dimension \$k=O({\textbackslash}log N{\textbackslash}polylog(n))\$, while (1) preserving the pairwise distances among the vectors to within any constant distortion and (2) being able to apply the transformation in time \$O(n{\textbackslash}log n)\$ on each vector. This improves on the best known \$N = {\textbackslash}exp({\textbackslash}tilde\{O\}(n{\textasciicircum}\{1/2\}))\$ achieved by Ailon and Liberty and \$N = {\textbackslash}exp({\textbackslash}tilde\{O\}(n{\textasciicircum}\{1/3\}))\$ by Ailon and Chazelle. The dependence in the distortion constant however is believed to be suboptimal and subject to further investigation. For constant distortion, this settles the open question posed by these authors up to a \${\textbackslash}polylog(n)\$ factor while considerably simplifying their constructions.},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Ailon, Nir and Liberty, Edo},
	month = may,
	year = {2010},
	note = {arXiv:1005.5513 [cs]},
	keywords = {Computer Science - Data Structures and Algorithms},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/6K2CNNIL/Ailon and Liberty - 2010 - Almost Optimal Unrestricted Fast Johnson-Lindenstr.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/G9DJ3FLD/1005.html:text/html},
}


@misc{cherapanamjeri_uniform_2022,
	title = {Uniform {Approximations} for {Randomized} {Hadamard} {Transforms} with {Applications}},
	url = {http://arxiv.org/abs/2203.01599},
	abstract = {Randomized Hadamard Transforms (RHTs) have emerged as a computationally efficient alternative to the use of dense unstructured random matrices across a range of domains in computer science and machine learning. For several applications such as dimensionality reduction and compressed sensing, the theoretical guarantees for methods based on RHTs are comparable to approaches using dense random matrices with i.i.d.{\textbackslash} entries. However, several such applications are in the low-dimensional regime where the number of rows sampled from the matrix is rather small. Prior arguments are not applicable to the high-dimensional regime often found in machine learning applications like kernel approximation. Given an ensemble of RHTs with Gaussian diagonals, \${\textbackslash}\{M{\textasciicircum}i{\textbackslash}\}\_\{i = 1\}{\textasciicircum}m\$, and any \$1\$-Lipschitz function, \$f: {\textbackslash}mathbb\{R\} {\textbackslash}to {\textbackslash}mathbb\{R\}\$, we prove that the average of \$f\$ over the entries of \${\textbackslash}\{M{\textasciicircum}i v{\textbackslash}\}\_\{i = 1\}{\textasciicircum}m\$ converges to its expectation uniformly over \${\textbackslash}{\textbar} v {\textbackslash}{\textbar} {\textbackslash}leq 1\$ at a rate comparable to that obtained from using truly Gaussian matrices. We use our inequality to then derive improved guarantees for two applications in the high-dimensional regime: 1) kernel approximation and 2) distance estimation. For kernel approximation, we prove the first {\textbackslash}emph\{uniform\} approximation guarantees for random features constructed through RHTs lending theoretical justification to their empirical success while for distance estimation, our convergence result implies data structures with improved runtime guarantees over previous work by the authors. We believe our general inequality is likely to find use in other applications.},
	urldate = {2024-07-04},
	publisher = {arXiv},
	author = {Cherapanamjeri, Yeshwanth and Nelson, Jelani},
	month = mar,
	year = {2022},
	note = {arXiv:2203.01599 [cs, stat]},
	keywords = {Computer Science - Computational Geometry, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/FNE9PTCP/Cherapanamjeri and Nelson - 2022 - Uniform Approximations for Randomized Hadamard Tra.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/ATDAI8FI/2203.html:text/html},
}



@article{chu_fft_1998,
	title = {{FFT} algorithms and their adaptation to parallel processing},
	volume = {284},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00243795},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379598100861},
	doi = {10.1016/S0024-3795(98)10086-1},
	language = {en},
	number = {1-3},
	urldate = {2024-07-08},
	journal = {Linear Algebra and its Applications},
	author = {Chu, Eleanor and George, Alan},
	month = nov,
	year = {1998},
	pages = {95--124},
}



@misc{rudelson_non-asymptotic_2010,
	title = {Non-asymptotic theory of random matrices: extreme singular values},
	shorttitle = {Non-asymptotic theory of random matrices},
	url = {http://arxiv.org/abs/1003.2990},
	abstract = {The classical random matrix theory is mostly focused on asymptotic spectral properties of random matrices as their dimensions grow to infinity. At the same time many recent applications from convex geometry to functional analysis to information theory operate with random matrices in fixed dimensions. This survey addresses the non-asymptotic theory of extreme singular values of random matrices with independent entries. We focus on recently developed geometric methods for estimating the hard edge of random matrices (the smallest singular value).},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Rudelson, Mark and Vershynin, Roman},
	month = apr,
	year = {2010},
	note = {arXiv:1003.2990 [math]},
	keywords = {Mathematics - Probability, Mathematics - Functional Analysis, 46B09, 60B20},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/WT3NIT7U/Rudelson and Vershynin - 2010 - Non-asymptotic theory of random matrices extreme .pdf:application/pdf},
}


@misc{murray_randomized_2023,
	title = {Randomized {Numerical} {Linear} {Algebra} : {A} {Perspective} on the {Field} {With} an {Eye} to {Software}},
	shorttitle = {Randomized {Numerical} {Linear} {Algebra}},
	url = {http://arxiv.org/abs/2302.11474},
	abstract = {Randomized numerical linear algebra - RandNLA, for short - concerns the use of randomization as a resource to develop improved algorithms for large-scale linear algebra computations. The origins of contemporary RandNLA lay in theoretical computer science, where it blossomed from a simple idea: randomization provides an avenue for computing approximate solutions to linear algebra problems more efficiently than deterministic algorithms. This idea proved fruitful in the development of scalable algorithms for machine learning and statistical data analysis applications. However, RandNLA's true potential only came into focus upon integration with the fields of numerical analysis and "classical" numerical linear algebra. Through the efforts of many individuals, randomized algorithms have been developed that provide full control over the accuracy of their solutions and that can be every bit as reliable as algorithms that might be found in libraries such as LAPACK. Recent years have even seen the incorporation of certain RandNLA methods into MATLAB, the NAG Library, NVIDIA's cuSOLVER, and SciKit-Learn. For all its success, we believe that RandNLA has yet to realize its full potential. In particular, we believe the scientific community stands to benefit significantly from suitably defined "RandBLAS" and "RandLAPACK" libraries, to serve as standards conceptually analogous to BLAS and LAPACK. This 200-page monograph represents a step toward defining such standards. In it, we cover topics spanning basic sketching, least squares and optimization, low-rank approximation, full matrix decompositions, leverage score sampling, and sketching data with tensor product structures (among others). Much of the provided pseudo-code has been tested via publicly available MATLAB and Python implementations.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Murray, Riley and Demmel, James and Mahoney, Michael W. and Erichson, N. Benjamin and Melnichenko, Maksim and Malik, Osman Asif and Grigori, Laura and Luszczek, Piotr and Dereziński, Michał and Lopes, Miles E. and Liang, Tianyu and Luo, Hengrui and Dongarra, Jack},
	month = apr,
	year = {2023},
	note = {arXiv:2302.11474 [cs, math]},
	keywords = {Computer Science - Mathematical Software, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/S8B3J2MF/Murray et al. - 2023 - Randomized Numerical Linear Algebra  A Perspectiv.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/A2FKSGVL/2302.html:text/html},
}


@article{liberty_randomized_2007,
	title = {Randomized algorithms for the low-rank approximation of matrices},
	volume = {104},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.0709640104},
	doi = {10.1073/pnas.0709640104},
	abstract = {We describe two recently proposed randomized algorithms for the construction of low-rank approximations to matrices, and demonstrate their application (
              inter alia
              ) to the evaluation of the singular value decompositions of numerically low-rank matrices. Being probabilistic, the schemes described here have a finite probability of failure; in most cases, this probability is rather negligible (10
              −17
              is a typical value). In many situations, the new procedures are considerably more efficient and reliable than the classical (deterministic) ones; they also parallelize naturally. We present several numerical examples to illustrate the performance of the schemes.},
	language = {en},
	number = {51},
	urldate = {2024-07-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Liberty, Edo and Woolfe, Franco and Martinsson, Per-Gunnar and Rokhlin, Vladimir and Tygert, Mark},
	month = dec,
	year = {2007},
	pages = {20167--20172},
	file = {Full Text:/Users/mmartine/Zotero/storage/H3VAC9F5/Liberty et al. - 2007 - Randomized algorithms for the low-rank approximati.pdf:application/pdf},
}


@incollection{beals_extensions_1984,
	address = {Providence, Rhode Island},
	title = {Extensions of {Lipschitz} mappings into a {Hilbert} space},
	volume = {26},
	isbn = {978-0-8218-5030-5 978-0-8218-7611-4},
	url = {http://www.ams.org/conm/026/},
	language = {en},
	urldate = {2024-07-10},
	booktitle = {Contemporary {Mathematics}},
	publisher = {American Mathematical Society},
	author = {Johnson, William B. and Lindenstrauss, Joram},
	editor = {Beals, Richard and Beck, Anatole and Bellow, Alexandra and Hajian, Arshag},
	year = {1984},
	doi = {10.1090/conm/026/737400},
	pages = {189--206},
}


@article{oseledets_tensor-train_2011,
	title = {Tensor-{Train} {Decomposition}},
	volume = {33},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/090752286},
	doi = {10.1137/090752286},
	language = {en},
	number = {5},
	urldate = {2024-04-03},
	journal = {SIAM Journal on Scientific Computing},
	author = {Oseledets, I. V.},
	month = jan,
	year = {2011},
	pages = {2295--2317},
}


@misc{ye_quantized_2024,
	title = {Quantized tensor networks for solving the {Vlasov}-{Maxwell} equations},
	url = {http://arxiv.org/abs/2311.07756},
	abstract = {The Vlasov-Maxwell equations provide an {\textbackslash}textit\{ab-initio\} description of collisionless plasmas, but solving them is often impractical because of the wide range of spatial and temporal scales that must be resolved and the high dimensionality of the problem. In this work, we present a quantum-inspired semi-implicit Vlasov-Maxwell solver that utilizes the quantized tensor network (QTN) framework. With this QTN solver, the cost of grid-based numerical simulation of size \$N\$ is reduced from \${\textbackslash}mathcal\{O\}(N)\$ to \${\textbackslash}mathcal\{O\}({\textbackslash}text\{poly\}(D))\$, where \$D\$ is the ``rank'' or ``bond dimension'' of the QTN and is typically set to be much smaller than \$N\$. We find that for the five-dimensional test problems considered here, a modest \$D=64\$ appears to be sufficient for capturing the expected physics despite the simulations using a total of \$N=2{\textasciicircum}\{36\}\$ grid points, {\textbackslash}edit\{which would require \$D=2{\textasciicircum}\{18\}\$ for full-rank calculations\}. Additionally, we observe that a QTN time evolution scheme based on the Dirac-Frenkel variational principle allows one to use somewhat larger time steps than prescribed by the Courant-Friedrichs-Lewy (CFL) constraint. As such, this work demonstrates that the QTN format is a promising means of approximately solving the Vlasov-Maxwell equations with significantly reduced cost.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Ye, Erika and Loureiro, Nuno},
	month = jun,
	year = {2024},
	note = {arXiv:2311.07756 [physics, physics:quant-ph]},
	keywords = {Physics - Computational Physics, Physics - Plasma Physics, Quantum Physics},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/KW6LX9E7/Ye and Loureiro - 2024 - Quantized tensor networks for solving the Vlasov-M.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/GLGIJUZ8/2311.html:text/html},
}


@article{baiardi_density_2020,
	title = {The {Density} {Matrix} {Renormalization} {Group} in {Chemistry} and {Molecular} {Physics}: {Recent} {Developments} and {New} {Challenges}},
	volume = {152},
	issn = {0021-9606, 1089-7690},
	shorttitle = {The {Density} {Matrix} {Renormalization} {Group} in {Chemistry} and {Molecular} {Physics}},
	url = {http://arxiv.org/abs/1910.00137},
	doi = {10.1063/1.5129672},
	abstract = {In the past two decades, the density matrix renormalization group (DMRG) has emerged as an innovative new method in quantum chemistry relying on a theoretical framework very different from that of traditional electronic structure approaches. The development of the quantum chemical DMRG has been remarkably fast: it has already become one of the reference approaches for large-scale multiconfigurational calculations. This perspective discusses the major features of DMRG, highlighting its strengths and weaknesses also in comparison to other novel approaches. The method is presented following its historical development, starting from its original formulation up to its most recent applications. Possible routes to recover dynamical correlation are discussed in detail. Emerging new fields of applications of DMRG are explored, in particular its time-dependent formulation and the application to vibrational spectroscopy.},
	number = {4},
	urldate = {2024-07-10},
	journal = {The Journal of Chemical Physics},
	author = {Baiardi, Alberto and Reiher, Markus},
	month = jan,
	year = {2020},
	note = {arXiv:1910.00137 [cond-mat, physics:physics]},
	keywords = {Condensed Matter - Strongly Correlated Electrons, Physics - Chemical Physics, Physics - Computational Physics},
	pages = {040903},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/2AKQAHQE/Baiardi and Reiher - 2020 - The Density Matrix Renormalization Group in Chemis.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/NCAKGY5G/1910.html:text/html},
}


@article{klus_tensor-based_2018,
	title = {Tensor-based dynamic mode decomposition},
	volume = {31},
	issn = {0951-7715, 1361-6544},
	url = {http://arxiv.org/abs/1606.06625},
	doi = {10.1088/1361-6544/aabc8f},
	abstract = {Dynamic mode decomposition (DMD) is a recently developed tool for the analysis of the behavior of complex dynamical systems. In this paper, we will propose an extension of DMD that exploits low-rank tensor decompositions of potentially high-dimensional data sets to compute the corresponding DMD modes and eigenvalues. The goal is to reduce the computational complexity and also the amount of memory required to store the data in order to mitigate the curse of dimensionality. The efficiency of these tensor-based methods will be illustrated with the aid of several different fluid dynamics problems such as the von K{\textbackslash}'arm{\textbackslash}'an vortex street and the simulation of two merging vortices.},
	number = {7},
	urldate = {2024-03-27},
	journal = {Nonlinearity},
	author = {Klus, Stefan and Gelß, Patrick and Peitz, Sebastian and Schütte, Christof},
	month = jul,
	year = {2018},
	note = {arXiv:1606.06625 [math]},
	keywords = {15A69, 37N10, 37L65, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis},
	pages = {3359--3380},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/KP9S5HAF/Klus et al. - 2018 - Tensor-based dynamic mode decomposition.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/7VP8MN2U/1606.html:text/html},
}

@article{li_tensor_2023,
	title = {Tensor {Train}-{Based} {Higher}-{Order} {Dynamic} {Mode} {Decomposition} for {Dynamical} {Systems}},
	volume = {11},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2227-7390},
	url = {https://www.mdpi.com/2227-7390/11/8/1809},
	doi = {10.3390/math11081809},
	abstract = {Higher-order dynamic mode decomposition (HODMD) has proved to be an efficient tool for the analysis and prediction of complex dynamical systems described by data-driven models. In the present paper, we propose a realization of HODMD that is based on the low-rank tensor decomposition of potentially high-dimensional datasets. It is used to compute the HODMD modes and eigenvalues to effectively reduce the computational complexity of the problem. The proposed extension also provides a more efficient realization of the ordinary dynamic mode decomposition with the use of the tensor-train decomposition. The high efficiency of the tensor-train-based HODMD (TT-HODMD) is illustrated by a few examples, including forecasting the load of a power system, which provides comparisons between TT-HODMD and HODMD with respect to the computing time and accuracy. The developed algorithm can be effectively used for the prediction of high-dimensional dynamical systems.},
	language = {en},
	number = {8},
	urldate = {2024-03-27},
	journal = {Mathematics},
	author = {Li, Keren and Utyuzhnikov, Sergey},
	month = apr,
	year = {2023},
	pages = {1809},
	file = {Full Text:/Users/mmartine/Zotero/storage/L7P6RW7K/Li and Utyuzhnikov - 2023 - Tensor Train-Based Higher-Order Dynamic Mode Decom.pdf:application/pdf},
}

@article{le_clainche_higher_2017,
	title = {Higher {Order} {Dynamic} {Mode} {Decomposition}},
	volume = {16},
	issn = {1536-0040},
	url = {https://epubs.siam.org/doi/10.1137/15M1054924},
	doi = {10.1137/15M1054924},
	language = {en},
	number = {2},
	urldate = {2024-03-27},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Le Clainche, Soledad and Vega, José M.},
	month = jan,
	year = {2017},
	pages = {882--925},
	file = {Accepted Version:/Users/mmartine/Zotero/storage/YLTW9VFJ/Le Clainche and Vega - 2017 - Higher Order Dynamic Mode Decomposition.pdf:application/pdf},
}


@article{cichocki_tensor_2014,
	title = {Tensor {Networks} for {Big} {Data} {Analytics} and {Large}-{Scale} {Optimization} {Problems}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1407.3124},
	doi = {10.48550/ARXIV.1407.3124},
	abstract = {In this paper we review basic and emerging models and associated algorithms for large-scale tensor networks, especially Tensor Train (TT) decompositions using novel mathematical and graphical representations. We discus the concept of tensorization (i.e., creating very high-order tensors from lower-order original data) and super compression of data achieved via quantized tensor train (QTT) networks. The purpose of a tensorization and quantization is to achieve, via low-rank tensor approximations "super" compression, and meaningful, compact representation of structured data. The main objective of this paper is to show how tensor networks can be used to solve a wide class of big data optimization problems (that are far from tractable by classical numerical methods) by applying tensorization and performing all operations using relatively small size matrices and tensors and applying iteratively optimized and approximative tensor contractions. Keywords: Tensor networks, tensor train (TT) decompositions, matrix product states (MPS), matrix product operators (MPO), basic tensor operations, tensorization, distributed representation od data optimization problems for very large-scale problems: generalized eigenvalue decomposition (GEVD), PCA/SVD, canonical correlation analysis (CCA).},
	urldate = {2024-04-03},
	author = {Cichocki, Andrzej},
	year = {2014},
	note = {Publisher: [object Object]
Version Number: 2},
	keywords = {FOS: Mathematics, Numerical Analysis (math.NA)},
}

@article{liu_incremental_2021,
	title = {An {Incremental} {Tensor}-{Train} {Decomposition} for {Cyber}-{Physical}-{Social} {Big} {Data}},
	volume = {7},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {2332-7790, 2372-2096},
	url = {https://ieeexplore.ieee.org/document/8449102/},
	doi = {10.1109/TBDATA.2018.2867485},
	number = {2},
	urldate = {2024-04-11},
	journal = {IEEE Transactions on Big Data},
	author = {Liu, Huazhong and Yang, Laurence T. and Guo, Yimu and Xie, Xia and Ma, Jianhua},
	month = jun,
	year = {2021},
	pages = {341--354},
}


@article{kressner_streaming_2023,
	title = {Streaming {Tensor} {Train} {Approximation}},
	volume = {45},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/22M1515045},
	doi = {10.1137/22M1515045},
	language = {en},
	number = {5},
	urldate = {2024-04-11},
	journal = {SIAM Journal on Scientific Computing},
	author = {Kressner, Daniel and Vandereycken, Bart and Voorhaar, Rik},
	month = oct,
	year = {2023},
	pages = {A2610--A2631},
	file = {Submitted Version:/Users/mmartine/Zotero/storage/SERCVLZV/Kressner et al. - 2023 - Streaming Tensor Train Approximation.pdf:application/pdf},
}

@misc{daas_randomized_2021,
	title = {Randomized algorithms for rounding in the {Tensor}-{Train} format},
	url = {http://arxiv.org/abs/2110.04393},
	abstract = {The Tensor-Train (TT) format is a highly compact low-rank representation for high-dimensional tensors. TT is particularly useful when representing approximations to the solutions of certain types of parametrized partial differential equations. For many of these problems, computing the solution explicitly would require an infeasible amount of memory and computational time. While the TT format makes these problems tractable, iterative techniques for solving the PDEs must be adapted to perform arithmetic while maintaining the implicit structure. The fundamental operation used to maintain feasible memory and computational time is called rounding, which truncates the internal ranks of a tensor already in TT format. We propose several randomized algorithms for this task that are generalizations of randomized low-rank matrix approximation algorithms and provide significant reduction in computation compared to deterministic TT-rounding algorithms. Randomization is particularly effective in the case of rounding a sum of TT-tensors (where we observe 20x speedup), which is the bottleneck computation in the adaptation of GMRES to vectors in TT format. We present the randomized algorithms and compare their empirical accuracy and computational time with deterministic alternatives.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Daas, Hussam Al and Ballard, Grey and Cazeaux, Paul and Hallman, Eric and Miedlar, Agnieszka and Pasha, Mirjeta and Reid, Tim W. and Saibaba, Arvind K.},
	month = oct,
	year = {2021},
	note = {arXiv:2110.04393 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
}


@misc{ma_cost-efficient_2022,
	title = {Cost-efficient {Gaussian} {Tensor} {Network} {Embeddings} for {Tensor}-structured {Inputs}},
	url = {http://arxiv.org/abs/2205.13163},
	abstract = {This work discusses tensor network embeddings, which are random matrices (\$S\$) with tensor network structure. These embeddings have been used to perform dimensionality reduction of tensor network structured inputs \$x\$ and accelerate applications such as tensor decomposition and kernel regression. Existing works have designed embeddings for inputs \$x\$ with specific structures, such that the computational cost for calculating \$Sx\$ is efficient. We provide a systematic way to design tensor network embeddings consisting of Gaussian random tensors, such that for inputs with more general tensor network structures, both the sketch size (row size of \$S\$) and the sketching computational cost are low. We analyze general tensor network embeddings that can be reduced to a sequence of sketching matrices. We provide a sufficient condition to quantify the accuracy of such embeddings and derive sketching asymptotic cost lower bounds using embeddings that satisfy this condition and have a sketch size lower than any input dimension. We then provide an algorithm to efficiently sketch input data using such embeddings. The sketch size of the embedding used in the algorithm has a linear dependence on the number of sketching dimensions of the input. Assuming tensor contractions are performed with classical dense matrix multiplication algorithms, this algorithm achieves asymptotic cost within a factor of \$O({\textbackslash}sqrt\{m\})\$ of our cost lower bound, where \$m\$ is the sketch size. Further, when each tensor in the input has a dimension that needs to be sketched, this algorithm yields the optimal sketching asymptotic cost. We apply our sketching analysis to inexact tensor decomposition optimization algorithms. We provide a sketching algorithm for CP decomposition that is asymptotically faster than existing work in multiple regimes, and show optimality of an existing algorithm for tensor train rounding.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Ma, Linjian and Solomonik, Edgar},
	month = may,
	year = {2022},
	note = {arXiv:2205.13163 [cs, math]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/9LBAPNGU/Ma and Solomonik - 2022 - Cost-efficient Gaussian Tensor Network Embeddings .pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/8G7NTVKR/2205.html:text/html},
}


@article{udell_why_2019,
	title = {Why {Are} {Big} {Data} {Matrices} {Approximately} {Low} {Rank}?},
	volume = {1},
	issn = {2577-0187},
	url = {https://epubs.siam.org/doi/10.1137/18M1183480},
	doi = {10.1137/18M1183480},
	language = {en},
	number = {1},
	urldate = {2024-07-11},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Udell, Madeleine and Townsend, Alex},
	month = jan,
	year = {2019},
	pages = {144--160},
}


@article{demmel_improved_2023,
	title = {An {Improved} {Analysis} and {Unified} {Perspective} on {Deterministic} and {Randomized} {Low}-{Rank} {Matrix} {Approximation}},
	volume = {44},
	issn = {0895-4798, 1095-7162},
	url = {https://epubs.siam.org/doi/10.1137/21M1391316},
	doi = {10.1137/21M1391316},
	language = {en},
	number = {2},
	urldate = {2024-07-11},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Demmel, James and Grigori, Laura and Rusciano, Alexander},
	month = jun,
	year = {2023},
	pages = {559--591},
}



@misc{fornace_column_2024,
	title = {Column and row subset selection using nuclear scores: algorithms and theory for {Nystr}{\textbackslash}"\{o\}m approximation, {CUR} decomposition, and graph {Laplacian} reduction},
	shorttitle = {Column and row subset selection using nuclear scores},
	url = {http://arxiv.org/abs/2407.01698},
	abstract = {Column selection is an essential tool for structure-preserving low-rank approximation, with wide-ranging applications across many fields, such as data science, machine learning, and theoretical chemistry. In this work, we develop unified methodologies for fast, efficient, and theoretically guaranteed column selection. First we derive and implement a sparsity-exploiting deterministic algorithm applicable to tasks including kernel approximation and CUR decomposition. Next, we develop a matrix-free formalism relying on a randomization scheme satisfying guaranteed concentration bounds, applying this construction both to CUR decomposition and to the approximation of matrix functions of graph Laplacians. Importantly, the randomization is only relevant for the computation of the scores that we use for column selection, not the selection itself given these scores. For both deterministic and matrix-free algorithms, we bound the performance favorably relative to the expected performance of determinantal point process (DPP) sampling and, in select scenarios, that of exactly optimal subset selection. The general case requires new analysis of the DPP expectation. Finally, we demonstrate strong real-world performance of our algorithms on a diverse set of example approximation tasks.},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {Fornace, Mark and Lindsey, Michael},
	month = jul,
	year = {2024},
	note = {arXiv:2407.01698 [cs, math, stat]},
	keywords = {Mathematics - Numerical Analysis, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/ZJPCXB9Q/Fornace and Lindsey - 2024 - Column and row subset selection using nuclear scor.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/C7YH3RZJ/2407.html:text/html},
}


@misc{de_damas_randomized_2024,
	title = {Randomized {Implicitly} {Restarted} {Arnoldi} method for the non-symmetric eigenvalue problem},
	url = {http://arxiv.org/abs/2407.03208},
	abstract = {In this paper, we introduce a randomized algorithm for solving the non-symmetric eigenvalue problem, referred to as randomized Implicitly Restarted Arnoldi (rIRA). This method relies on using a sketch-orthogonal basis during the Arnoldi process while maintaining the Arnoldi relation and exploiting a restarting scheme to focus on a specific part of the spectrum. We analyze this method and show that it retains useful properties of the Implicitly Restarted Arnoldi (IRA) method, such as restarting without adding errors to the Ritz pairs and implicitly applying polynomial filtering. Experiments are presented to validate the numerical efficiency of the proposed randomized eigenvalue solver.},
	urldate = {2024-07-15},
	publisher = {arXiv},
	author = {de Damas, Jean-Guillaume and Grigori, Laura},
	month = jul,
	year = {2024},
	note = {arXiv:2407.03208 [cs, math]},
	keywords = {65F10, 65F15, 65F25, 15B52, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/GE2MRZMH/de Damas and Grigori - 2024 - Randomized Implicitly Restarted Arnoldi method for.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/8YCMUQTB/2407.html:text/html},
}


@misc{nakatsukasa_fast_2020,
	title = {Fast and stable randomized low-rank matrix approximation},
	url = {http://arxiv.org/abs/2009.11392},
	abstract = {Randomized SVD has become an extremely successful approach for efficiently computing a low-rank approximation of matrices. In particular the paper by Halko, Martinsson, and Tropp (SIREV 2011) contains extensive analysis, and has made it a very popular method. The typical complexity for a rank-\$r\$ approximation of \$m{\textbackslash}times n\$ matrices is \$O(mn{\textbackslash}log n+(m+n)r{\textasciicircum}2)\$ for dense matrices. The classical Nystr\{{\textbackslash}"o\}m method is much faster, but applicable only to positive semidefinite matrices. This work studies a generalization of Nystr\{{\textbackslash}"o\}m method applicable to general matrices, and shows that (i) it has near-optimal approximation quality comparable to competing methods, (ii) the computational cost is the near-optimal \$O(mn{\textbackslash}log n+r{\textasciicircum}3)\$ for dense matrices, with small hidden constants, and (iii) crucially, it can be implemented in a numerically stable fashion despite the presence of an ill-conditioned pseudoinverse. Numerical experiments illustrate that generalized Nystr\{{\textbackslash}"o\}m can significantly outperform state-of-the-art methods, especially when \$r{\textbackslash}gg 1\$, achieving up to a 10-fold speedup. The method is also well suited to updating and downdating the matrix.},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Nakatsukasa, Yuji},
	month = sep,
	year = {2020},
	note = {arXiv:2009.11392 [cs, math]},
	keywords = {65F55, 68W20, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/mmartine/Zotero/storage/MP2R2LFG/Nakatsukasa - 2020 - Fast and stable randomized low-rank matrix approxi.pdf:application/pdf;arXiv.org Snapshot:/Users/mmartine/Zotero/storage/C9AB5X7L/2009.html:text/html},
}




@article{brubeck_vandermonde_2021,
	title = {Vandermonde with {Arnoldi}},
	volume = {63},
	issn = {0036-1445, 1095-7200},
	url = {https://epubs.siam.org/doi/10.1137/19M130100X},
	doi = {10.1137/19M130100X},
	language = {en},
	number = {2},
	urldate = {2024-10-30},
	journal = {SIAM Review},
	author = {Brubeck, Pablo D. and Nakatsukasa, Yuji and Trefethen, Lloyd N.},
	month = jan,
	year = {2021},
	pages = {405--415},
	file = {Full Text:/Users/mmartine/Zotero/storage/7NAMX9KH/Brubeck et al. - 2021 - Vandermonde with Arnoldi.pdf:application/pdf},
}

@article{barnett_how_2022,
	title = {How {Exponentially} {Ill}-{Conditioned} {Are} {Contiguous} {Submatrices} of the {Fourier} {Matrix}?},
	volume = {64},
	issn = {0036-1445, 1095-7200},
	url = {https://epubs.siam.org/doi/10.1137/20M1336837},
	doi = {10.1137/20M1336837},
	language = {en},
	number = {1},
	urldate = {2024-10-30},
	journal = {SIAM Review},
	author = {Barnett, Alex H.},
	month = feb,
	year = {2022},
	pages = {105--131},
	file = {Submitted Version:/Users/mmartine/Zotero/storage/M2LFDSP2/Barnett - 2022 - How Exponentially Ill-Conditioned Are Contiguous S.pdf:application/pdf},
}

@article{huybrechs_fourier_2010,
	title = {On the {Fourier} {Extension} of {Nonperiodic} {Functions}},
	volume = {47},
	issn = {0036-1429, 1095-7170},
	url = {http://epubs.siam.org/doi/10.1137/090752456},
	doi = {10.1137/090752456},
	language = {en},
	number = {6},
	urldate = {2024-10-30},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Huybrechs, Daan},
	month = jan,
	year = {2010},
	pages = {4326--4355},
	file = {Accepted Version:/Users/mmartine/Zotero/storage/JB3CQ2DW/Huybrechs - 2010 - On the Fourier Extension of Nonperiodic Functions.pdf:application/pdf},
}


@article{boyd_comparison_2002,
	title = {A {Comparison} of {Numerical} {Algorithms} for {Fourier} {Extension} of the {First}, {Second}, and {Third} {Kinds}},
	volume = {178},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999102970233},
	doi = {10.1006/jcph.2002.7023},
	language = {en},
	number = {1},
	urldate = {2024-10-30},
	journal = {Journal of Computational Physics},
	author = {Boyd, John P.},
	month = may,
	year = {2002},
	pages = {118--160},
}
